{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Deep Learning - a use case based approach to understand deep neural networks\n",
    "\n",
    "### Umberto Michelucci\n",
    "\n",
    "Buy the book: https://www.apress.com/us/book/9781484237892"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) Umberto Michelucci 2018-2019 - umberto.michelucci@gmail.com \n",
    "\n",
    "github repository: https://github.com/Apress/applied-deep-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zalando dataset\n",
    "\n",
    "https://www.kaggle.com/zalando-research/fashionmnist/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context**\n",
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\n",
    "\n",
    "The original MNIST dataset contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. \"If it doesn't work on MNIST, it won't work at all\", they said. \"Well, if it does work on MNIST, it may still fail on others.\"\n",
    "Zalando seeks to replace the original MNIST dataset\n",
    "\n",
    "**Content**\n",
    "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
    "\n",
    "To locate a pixel on the image, suppose that we have decomposed x as x = i * 28 + j, where i and j are integers between 0 and 27. The pixel is located on row i and column j of a 28 x 28 matrix. \n",
    "For example, pixel31 indicates the pixel that is in the fourth column from the left, and the second row from the top, as in the ascii-diagram below. \n",
    "\n",
    "**Labels**\n",
    "Each training and test example is assigned to one of the following labels:\n",
    "- 0 T-shirt/top\n",
    "- 1 Trouser\n",
    "- 2 Pullover\n",
    "- 3 Dress\n",
    "- 4 Coat\n",
    "- 5 Sandal\n",
    "- 6 Shirt\n",
    "- 7 Sneaker\n",
    "- 8 Bag\n",
    "- 9 Ankle boot \n",
    "\n",
    "**TL;DR**\n",
    "Each row is a separate image \n",
    "Column 1 is the class label. \n",
    "Remaining columns are pixel numbers (784 total). \n",
    "Each value is the darkness of the pixel (1 to 255)\n",
    "\n",
    "**Acknowledgements**\n",
    "Original dataset was downloaded from https://github.com/zalandoresearch/fashion-mnist\n",
    "Dataset was converted to CSV with this script: https://pjreddie.com/projects/mnist-in-csv/\n",
    "\n",
    "**License**\n",
    "The MIT License (MIT) Copyright © [2017] Zalando SE, https://tech.zalando.com\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_name(idx):\n",
    "    if (idx == 0):\n",
    "        return '(0) T-shirt/top'\n",
    "    elif (idx == 1):\n",
    "        return '(1) Trouser'\n",
    "    elif (idx == 2):\n",
    "        return '(2) Pullover'\n",
    "    elif (idx == 3):\n",
    "        return '(3) Dress'\n",
    "    elif (idx == 4):\n",
    "        return '(4) Coat'\n",
    "    elif (idx == 5):\n",
    "        return '(5) Sandal'\n",
    "    elif (idx == 6):\n",
    "        return '(6) Shirt'\n",
    "    elif (idx == 7):\n",
    "        return '(7) Sneaker'\n",
    "    elif (idx == 8):\n",
    "        return '(8) Bag'\n",
    "    elif (idx == 9):\n",
    "        return '(9) Ankle boot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_element_with_label (data, lbls, lbl):\n",
    "    subset = data[lbls == lbl]\n",
    "    return np.random.choice(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REMEMBER: unzip the zip files before running the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('fashion-mnist_train.csv', header = 0)\n",
    "data_test = pd.read_csv('fashion-mnist_test.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function to get a random element from a subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_train['label'].values.reshape(1, 60000)\n",
    "\n",
    "labels_ = np.zeros((60000, 10))\n",
    "labels_[np.arange(60000), labels] = 1\n",
    "labels_ = labels_.transpose()\n",
    "\n",
    "\n",
    "train = data_train.drop('label', axis=1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels_.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = data_test['label'].values.reshape(1, 10000)\n",
    "\n",
    "labels_test_ = np.zeros((10000, 10))\n",
    "labels_test_[np.arange(10000), labels_test] = 1\n",
    "labels_test_ = labels_test_.transpose()\n",
    "\n",
    "\n",
    "test = data_test.drop('label', axis=1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's normalize the training data dividing by 255.0 to get the values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train / 255.0)\n",
    "test = np.array(test / 255.0)\n",
    "labels_ = np.array(labels_)\n",
    "labels_test_ = np.array(labels_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One example of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 5\n",
    "plt.imshow(train[:,idx].reshape(28,28), cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
    "plt.axis(\"on\")\n",
    "plt.title(get_label_name(labels[:,idx]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's check some random sample of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe following function will return one numpy array (one column) with an example of a choosen label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_element_with_label (data, lbls, lbl):\n",
    "    tmp = lbls == lbl\n",
    "    subset = data[:,tmp.flatten()]\n",
    "    return subset[:,randint(1,subset.shape[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create an array with a column for each label (one-hot encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code create a numpy array where in column 0 you will find an example of label 0, in column 1 of label 1 and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_overview = np.empty([784,10])\n",
    "for i in range (0,10):\n",
    "    col = get_random_element_with_label(train, labels, i)\n",
    "    labels_overview[:,i] = col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot one example of each type (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(15,15));\n",
    "count = 1\n",
    "for i in range(0,10):\n",
    "    plt.subplot(5,2,count)\n",
    "    count = count + 1\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.title(get_label_name(i))\n",
    "    some_digit_image = labels_overview[:,i].reshape(28,28)\n",
    "    plt.imshow(some_digit_image, cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
    "    plt.axis(\"off\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network with 1 layer and 15 neurons - with consant learning rate $\\gamma$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 784\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Number of neurons in the layers\n",
    "n1 = 15 # Number of neurons in layer 1\n",
    "n2 = 10 # Number of neurons in output layer \n",
    "\n",
    "cost_history = np.empty(shape=[1], dtype = float)\n",
    "learning_rate = tf.placeholder(tf.float32, shape=())\n",
    "\n",
    "X = tf.placeholder(tf.float32, [n_dim, None])\n",
    "Y = tf.placeholder(tf.float32, [10, None])\n",
    "W1 = tf.Variable(tf.truncated_normal([n1, n_dim], stddev=.1)) \n",
    "b1 = tf.Variable(tf.constant(0.1, shape = [n1,1]) )\n",
    "W2 = tf.Variable(tf.truncated_normal([n2, n1], stddev=.1)) \n",
    "b2 = tf.Variable(tf.constant(0.1, shape = [n2,1])) \n",
    "                 \n",
    "# Let's build our network...\n",
    "Z1 = tf.nn.relu(tf.matmul(W1, X) + b1) # n1 x n_dim * n_dim x n_obs = n1 x n_obs\n",
    "Z2 = tf.matmul(W2, Z1) + b2 # n2 x n1 * n1 * n_obs = n2 x n_obs\n",
    "y_ = tf.nn.softmax(Z2,0) # n2 x n_obs (10 x None)\n",
    "\n",
    "cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "cost_history = []\n",
    "for epoch in range(100+1):\n",
    "    for i in range(0, train.shape[1], 50):\n",
    "        X_train_mini = train[:,i:i + 50]\n",
    "        y_train_mini = labels_[:,i:i + 50]\n",
    "\n",
    "        sess.run(optimizer, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: 0.001})\n",
    "    cost_ = sess.run(cost, feed_dict={ X:train, Y: labels_, learning_rate: 0.001})\n",
    "    cost_history = np.append(cost_history, cost_)\n",
    "\n",
    "    if (epoch % 20 == 0):\n",
    "        print(\"Reached epoch\",epoch,\"cost J =\", cost_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "print (\"Accuracy:\", accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "print (\"Accuracy:\", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network with 3 layers and 14 neurons in each layer, with Learning rate decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dim = 784\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Number of neurons in the layers\n",
    "n1 = 14 # Number of neurons in layer 1\n",
    "n2 = 14 # Number of neurons in layer 2 \n",
    "n3 = 14\n",
    "n4 = 10\n",
    "#n5 = 10 # Neurons for the softmax function\n",
    "\n",
    "cost_history = np.empty(shape=[0], dtype = float)\n",
    "learning_rate = tf.placeholder(tf.float32, shape=())\n",
    "\n",
    "stddev_f = 0.1\n",
    "\n",
    "tf.set_random_seed(5)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [n_dim, None])\n",
    "Y = tf.placeholder(tf.float32, [10, None])\n",
    "W1 = tf.Variable(tf.random_normal([n1, n_dim], stddev=stddev_f)) \n",
    "b1 = tf.Variable(tf.constant(0.0, shape = [n1,1]) )\n",
    "W2 = tf.Variable(tf.random_normal([n2, n1], stddev=stddev_f)) \n",
    "b2 = tf.Variable(tf.constant(0.0, shape = [n2,1])) \n",
    "W3 = tf.Variable(tf.random_normal([n3,n2], stddev = stddev_f))\n",
    "b3 = tf.Variable(tf.constant(0.0, shape = [n3,1]))\n",
    "W4 = tf.Variable(tf.random_normal([n4,n3], stddev = stddev_f))\n",
    "b4 = tf.Variable(tf.constant(0.0, shape = [n4,1]))\n",
    "                 \n",
    "# Let's build our network...\n",
    "Z1 = tf.nn.relu(tf.matmul(W1, X) + b1) # n1 x n_dim * n_dim x n_obs = n1 x n_obs\n",
    "Z2 = tf.nn.relu(tf.matmul(W2, Z1) + b2) # n2 x n1 * n1 * n_obs = n2 x n_obs\n",
    "Z3 = tf.nn.relu(tf.matmul(W3, Z2) + b3)\n",
    "Z4 = tf.matmul(W4, Z3) + b4\n",
    "y_ = tf.nn.softmax(Z4,0) # n2 x n_obs (10 x None)\n",
    "\n",
    "\n",
    "cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "learning_r = 0.3\n",
    "minibatch_size = 50\n",
    "    \n",
    "cost_history = []\n",
    "for epoch in range(100+1):\n",
    "    #print (epoch, ' ', learning_r)\n",
    "    for i in range(0, train.shape[1], minibatch_size):\n",
    "        X_train_mini = train[:,i:i + minibatch_size]\n",
    "        y_train_mini = labels_[:,i:i + minibatch_size]\n",
    "\n",
    "        #sess.run(optimizer, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})\n",
    "        sess.run(optimizer, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})\n",
    "    cost_ = sess.run(cost, feed_dict={ X:train, Y: labels_})\n",
    "    cost_history = np.append(cost_history, cost_)\n",
    "    \n",
    "    ##### YOUR CODE HERE\n",
    "    \n",
    "    if (epoch % 10 == 0):\n",
    "        print(\"Reached epoch\",epoch,\"cost J =\", cost_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "print (\"Accuracy:\", accuracy.eval({X: train, \n",
    "                                   Y: labels_, learning_rate: 0.001}, session = sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "print (\"Accuracy:\", accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', family='arial')\n",
    "plt.rc('xtick', labelsize='x-small')\n",
    "plt.rc('ytick', labelsize='x-small')\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "fig = plt.figure(figsize=(3.9, 3.1))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(cost_historyL3, ls='-', color = 'black', \n",
    "        label = '3 layers, 14 neurons ($Q=11560$), Batch size = 30, \\n$\\gamma = step wise$')\n",
    "ax.plot(cost_history15, ls='--', color = 'black', \n",
    "        label = '1 layer, 15 neurons ($Q=11935$), Batch size = 50, \\n$\\gamma = 0.05$')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('Cost function $J$')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlim((0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: implement a network with $\\gamma$ with inverse time decay by hand - difficulty hard/FUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the learning code decay adding code to the network with 3 layers. The matematical formula you want to implement is\n",
    "\n",
    "$$\n",
    "\\gamma = \\frac{\\gamma_0}{1+\\nu j}\n",
    "$$\n",
    "\n",
    "where $j$ is the number of iterations (**not epochs**).\n",
    "\n",
    "**hint** you can use a code similar to this (this code is not tested and it serves only as guidelines)\n",
    "    \n",
    "    m = train.shape[1]\n",
    "    number_of_batches = m / minibatch_size\n",
    "    learning_r = initial_learning_rate / (1.0 + nu * (epoch * number_of_batches + i)) \n",
    "\n",
    "where ```i``` is the iteration done during the mini_batch feeding, and ```number_of_batches``` is the number of observations divided by the size of the mini batches. For examples if you have ```m = 60000``` and ```minibatch_size = 50``` then ```number_of_batches = 1200```.\n",
    "\n",
    "Update the code part\n",
    "    ##### YOUR CODE HERE\n",
    "in the evaluation phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Implement a network with $\\gamma$ inverse time decay with tensorflow - difficulty hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code above with 3 layers implement a network with exponential decay and see which parameters are working bext to help converging. Compare your result with the plain gradient descent version available at the beginning of the notebook.\n",
    "\n",
    "To implement it you can use the call ```tf.train.inverse_time_decay ()``` and check its documentation at https://goo.gl/fiE2ML.\n",
    "\n",
    "**hint** To implement it you can use the following code in the building phase of your network. Note that you need to create the optimizer code in a slightly different way.\n",
    "\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.1\n",
    "    global_step = tf.Variable(0, trainable = False)\n",
    "    learning_rate_decay = tf.train.inverse_time_decay(initial_learning_rate,\n",
    "    global_step, decay_steps, decay_rate)\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate_decay).\n",
    "    minimize(cost, global_step = global_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
