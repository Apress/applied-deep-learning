{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks and Deep Learning for Life Sciences and Health Applications - An introductory course about theoretical fundamentals, case studies and implementations in python and tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) Umberto Michelucci 2018 - umberto.michelucci@gmail.com \n",
    "\n",
    "github repository: https://github.com/michelucci/dlcourse2018_students\n",
    "\n",
    "Fall Semester 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_name(idx):\n",
    "    if (idx == 0):\n",
    "        return '(0) T-shirt/top'\n",
    "    elif (idx == 1):\n",
    "        return '(1) Trouser'\n",
    "    elif (idx == 2):\n",
    "        return '(2) Pullover'\n",
    "    elif (idx == 3):\n",
    "        return '(3) Dress'\n",
    "    elif (idx == 4):\n",
    "        return '(4) Coat'\n",
    "    elif (idx == 5):\n",
    "        return '(5) Sandal'\n",
    "    elif (idx == 6):\n",
    "        return '(6) Shirt'\n",
    "    elif (idx == 7):\n",
    "        return '(7) Sneaker'\n",
    "    elif (idx == 8):\n",
    "        return '(8) Bag'\n",
    "    elif (idx == 9):\n",
    "        return '(9) Ankle boot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv('fashion-mnist_train.csv', header = 0)\n",
    "data_test = pd.read_csv('fashion-mnist_test.csv', header = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 785)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data_train['label'].values.reshape(1, 60000)\n",
    "\n",
    "labels_ = np.zeros((60000, 10))\n",
    "labels_[np.arange(60000), labels] = 1\n",
    "labels_ = labels_.transpose()\n",
    "\n",
    "\n",
    "train = data_train.drop('label', axis=1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000)\n",
      "(784, 60000)\n"
     ]
    }
   ],
   "source": [
    "print(labels_.shape)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test = data_test['label'].values.reshape(1, 10000)\n",
    "\n",
    "labels_test_ = np.zeros((10000, 10))\n",
    "labels_test_[np.arange(10000), labels_test] = 1\n",
    "labels_test_ = labels_test_.transpose()\n",
    "\n",
    "\n",
    "test = data_test.drop('label', axis=1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = np.array(train / 255.0)\n",
    "test = np.array(test / 255.0)\n",
    "labels_ = np.array(labels_)\n",
    "labels_test_ = np.array(labels_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAFpBJREFUeJzt3XuQ1eV5B/DvVwTlsiq6gAuSIigQShN0VqBEjcSoSGI0MzUTnDFkxinRJrWZyUzj+Ee17SR1Ok3UmaZpsTjRTGLKRKO2ERsvAYwjxIVylUsQl+sCy/0OgX36x/42WXF/z7Oc+/J+PzPM7p7vec959+w+nD3nvdHMICLpOa/aHRCR6lDxiyRKxS+SKBW/SKJU/CKJUvGLJErFfw4g+U8kv9nN675Aclq5+yS1T8Xfw5EcBOArAP6ji+wRkkbys50ufgzAd4LbvIjkEyQ3kzxMckP2dX2RfR2R9ef8Ym5HSkPF3/N9FcArZnas84UkRwH4CwAtnS83s98CuIhkY1c3RrIPgDcA/CmAaQAuAjAFwB4AE0vdeakeFX/PdzuABV1c/q8Avg3gZBfZfACfy7m9rwD4GIAvmtl7ZtZmZrvM7B/N7BUAIPlxkvNJ7ie5muQXOhqT/BzJ/yN5kOQWko92uu2F2cf92V8Uf35W36mUlIq/5/szAOs6X0DybgAnO4q1C2sAfDIn+yyAV83scFchyd4A/hvArwAMBvDXAH5Cckx2lSNo/w/kErT/B/MAybuy7Mbs4yVmNsDM3om+OSkfFX/PdwmAQx1fkBwA4LsAvDcAD2XtunIZznipcIbJAAYAeMzMTprZmwD+B8AMADCz+Wa2MvuLYQWA5wB8urvfjFSOir/n2wegrtPXfw/gx2b2gdOmDsD+nGwPgAan7VAAW8ysrdNlmwAMAwCSk0j+mmQryQMA7gdQ1BuFUh4q/p5vBYDRnb6+GcCDJHeQ3AFgOIC5JL/d6TofB7A85/ZeB3Abyf45+XYAw0l2/t35GIBt2ec/BfAygOFmdjGAfwfALNMS0hqi4u/5XsGH/6y+GcB4ABOyf9sBfA3ADzpd59MA5uXc3o8BbAHwPMmxJM8jeRnJh0lOB7AY7a/r/5Zkb5I3AbgDwM+y9nUA9prZcZITAdzT6bZbAbQBGFnwdyslo+Lv+Z4FMJ1kXwAwsz1mtqPjH4DTAPZ1vIFH8joAR7Ihv48wsxNof9NvLYDXABwE8Fu0/+m+2MxOAvgC2kcZdgP4NwBfMbO12U38FYB/IHkIwN8BmNvpto+ifY7B29lIweRSPhBydqjNPHo+kt8FsMvMnujGdZ8HMMcZCZBEqPhFEqU/+0USpeIXSZSKXyRRFV1dVV9fbyNGjCjLbVfzvQuSbl5s36Lbl67V8vtZ5fqZNjc3Y/fu3d268aKKP1sX/iSAXgD+08we864/YsQINDU1FXOXuX7/+9+7eTkL9Lzz/D+g2tra3Dy67/PP939M0f33VMU+bqdOncrNevXqVVCfunvfkd69exfVPk9jY5eLNbtU8G8NyV5onzhyO4BxAGaQHFfo7YlIZRXzlDERwAYz25hN/PgZgDtL0y0RKbdiin8Y2qeBdtiaXfYhJGeRbCLZ1NraWsTdiUgpFVP8Xb2I/sgLITObbWaNZtY4aNCgIu5OREqpmOLfivYVYx2uQPsiEhHpAYop/ncBXE3yymzfty+jfSmniPQABQ/1mdkpkt8A8L9oH+p72sxWl6xnZ6nYobxi2h8/ftxt27dvXzcvdqjOey/lE5/4hNt2+vTpbj5kyBA3X73a/5Hv27cvN1uwoKutB/+o2MfFG847evSo27bYobieMDejqHH+bGWYVoeJ9EDn5uwQEQmp+EUSpeIXSZSKXyRRKn6RRKn4RRKl01Iz0bjs6dOnc7P+/fO2uO+e7dv9iZFz585180WLFuVmt9xyi9t28eLFbn7w4EE3HzbsI8s5PuS6667LzWbMmOG2HT9+vJvfd999bt7QkH/2SL9+/dy2EW+5MFDbewl00DO/SKJU/CKJUvGLJErFL5IoFb9IolT8IonSUF8m2v33wgsvzM0OHDjgtn3xxRfdfPnyvNOy223dutXNhw4dmpvV19e7baOlq/v373fzaCt2b0gtGiLdsGGDmz/xhH80ofczjYYZvSFKIN5R2RsarhV65hdJlIpfJFEqfpFEqfhFEqXiF0mUil8kUSp+kUSdM+P80RLKKPfG8SMPPPCAm1911VVuPnjwYDcfMGCAm3vbb7/++utu21GjRrn5xIkT3fyXv/ylm3vzDOrq6ty20Um6F1xwgZtfdNFFudmTTz7ptr3//vvd/Prrr3fzaJy/2FOCS0HP/CKJUvGLJErFL5IoFb9IolT8IolS8YskSsUvkqhzZpw/Os452mo5MmfOnILbRnsFbNy40c0nT57s5t5x0ydPnnTbHj582M1XrFjh5pG2trbcLNr2OzpGe926dW7uzd2Ivu/HH3/czaNx/nP+iG6SzQAOATgN4JSZNZaiUyJSfqV45p9qZrtLcDsiUkF6zS+SqGKL3wD8iuQSkrO6ugLJWSSbSDa1trYWeXciUirFFv+nzOxaALcD+DrJG8+8gpnNNrNGM2scNGhQkXcnIqVSVPGb2fbs4y4AvwDgLwETkZpRcPGT7E+yruNzALcCWFWqjolIeRXzbv8QAL/IxjPPB/BTM3u1JL0qQLReP1r7HfHG+fv27eu2jdZ2Ry+Hli1b5ubePIFor4Bo/kOfPn3cPNq/fu/evbnZggUL3LbHjx9382gegHf0efR9RecVRKLzEGpBwcVvZhsBfLKEfRGRCtJQn0iiVPwiiVLxiyRKxS+SKBW/SKLOmSW93tLRUti2bVtuNnPmTLftkSNH3Dw6gjsaCvSG86ZOneq2ffVVf3R2y5Ytbh71zbv/Vav8aSFLly51c+/4b8B/3KPt1KMtz70hTAC49NJL3bwW6JlfJFEqfpFEqfhFEqXiF0mUil8kUSp+kUSp+EUSdc6M80dLeiPRslnvuOdoa+6BAwcW1KcOU6ZMcXPvmOz333/fbXvgwAE3HzlypJt/8MEHbu6Nh0fbuo0fP97No2Ou33nnndzs4MGDbtvocYmOJr/33nvdvBbomV8kUSp+kUSp+EUSpeIXSZSKXyRRKn6RRKn4RRJ1zozzF3sk8qJFi9zc20Z63rx5bttx48a5uXeUNAAsX77czb3jpqP19tHjFq3nj+Y4eLcfbWm+cuVKNx89erSbe3sweBkQb/UezZ/oCfTML5IoFb9IolT8IolS8YskSsUvkigVv0iiVPwiiTpnxvmLtXnzZjf3jlyOxoyjsfRovHr+/PluPmnSpNzshhtucNuuXr3azd9++203HzNmjJuPHTs2N4vW87/00ktufuzYMTf3jg+P5if079/fzXfs2OHmPUH4zE/yaZK7SK7qdNmlJF8j+bvsY3G7VYhIxXXnz/4fAZh2xmUPAXjDzK4G8Eb2tYj0IGHxm9lCAGfuxXQngGeyz58BcFeJ+yUiZVboG35DzKwFALKPuYfFkZxFsolkU/QaT0Qqp+zv9pvZbDNrNLPGaJGJiFROocW/k2QDAGQfd5WuSyJSCYUW/8sAOs6lngnAH5MRkZoTjvOTfA7ATQDqSW4F8AiAxwDMJXkfgM0A7i5nJ7vjvPOKewXT3Nzs5t44f3TfmzZtcvPPf/7zbh6NOXvr+aPzCHbu3Onm9fX1bn7kyBE398bi9+3b57YdMGCAm996661uvnTp0twsmt/g/byB+PelJwiL38xm5EQ3l7gvIlJBmt4rkigVv0iiVPwiiVLxiyRKxS+SqHNmSa+3fLM7oqOmo62cPd6230C8DfQ999zj5gsXLszNvOEuwD96HIiH26KhPm/b8ehxufHGG908+pn8/Oc/z82iobxo+Hb79u1u3hPomV8kUSp+kUSp+EUSpeIXSZSKXyRRKn6RRKn4RRJ1zozzF2vPnj1uPnhw7k5lOHnypNs2Oop63bp1bt7S0uLm3jbUR48eddv26tXLzfv16+fmp06dcvMDBw7kZpdcconb1nvMAeC9995zc2977Wi79OhndujQITfvCfTML5IoFb9IolT8IolS8YskSsUvkigVv0iiVPwiidI4f2b//v1u3tDQkJtFR3BH8wCuuuoqN1+/fr2bL1q0KDcbONA/QDlajx+1j9bke9uKR22XLFni5n369HHzurq63Cz6mbW1tbl5sVvF14Ke/x2ISEFU/CKJUvGLJErFL5IoFb9IolT8IolS8YskKplx/hMnTri5tyYe8Ne9R20vv/xyN//MZz7j5m+99ZabT5o0KTebPHmy2/app55y82i9fjSHYcSIEblZNL9h/vz5bn7HHXe4ubdfQPT7EM0hSGKcn+TTJHeRXNXpskdJbiO5LPs3vbzdFJFS685/Xz8CMK2Lyx83swnZv1dK2y0RKbew+M1sIYC9FeiLiFRQMS9cvkFyRfayIHcCOMlZJJtINrW2thZxdyJSSoUW/w8BjAIwAUALgO/lXdHMZptZo5k1Dho0qMC7E5FSK6j4zWynmZ02szYATwGYWNpuiUi5FVT8JDuvb/0igFV51xWR2hSO85N8DsBNAOpJbgXwCICbSE4AYACaAXytjH0siZ07d7p5tE+7N84fjYWPGjXKzaO98aMx5eHDh+dmUd+i277gggvcPJrjMHbs2NzsmmuucduuXbvWzaO9CK699trcbOXKlW7baP5C9Lj2BGHxm9mMLi6eU4a+iEgF9fxpSiJSEBW/SKJU/CKJUvGLJErFL5KoZJb0RltzR8Nxnr59+7r5mDFj3Dw67tk7ahoA9u7NX3oRDeV521sDwLFjx9w8Wvr65ptv5mbLly93227dutXNoyO8x40bl5stXbrUbRs9buef75fO7t273by+vt7NK0HP/CKJUvGLJErFL5IoFb9IolT8IolS8YskSsUvkqhkxvk3b97s5mbm5t64b7S0tHfv3m6+apW/HcLFF1/s5jfccENu9u6777pto75HojkO3lJqbxweAPr37+/m3vwGwF+OHB3R7S3hBuLvu6Wlxc01zi8iVaPiF0mUil8kUSp+kUSp+EUSpeIXSZSKXyRRyYzzR2vmo3Xp3jyAIUOGFNSnDvv27XPzaItr76jrNWvWuG2jvQKi48WjLa4vvPDC3KyhoSE3A+J5AAsWLHDzjRs35mYDB+aeMAcAOHr0qJtfccUVbl4L4/gRPfOLJErFL5IoFb9IolT8IolS8YskSsUvkigVv0iiunNE93AAzwK4HEAbgNlm9iTJSwH8F4ARaD+m+0tm5g9YV9GePXvcPFqf7c0DiMZ8IwMGDHDz9evXu7m3bj2aQxAdsR0dRR3tVeDtF7Blyxa3bXRmQLRm3tsPINojIZq/EP2+9IQjvLvzzH8KwLfM7OMAJgP4OslxAB4C8IaZXQ3gjexrEekhwuI3sxYzW5p9fgjAGgDDANwJ4Jnsas8AuKtcnRSR0jur1/wkRwC4BsBiAEPMrAVo/w8CgH92kojUlG4XP8kBAJ4H8E0zO3gW7WaRbCLZ1NraWkgfRaQMulX8JHujvfB/YmYvZBfvJNmQ5Q0AdnXV1sxmm1mjmTUOGjSoFH0WkRIIi5/t25zOAbDGzL7fKXoZwMzs85kAXip990SkXLqzpPdTAO4FsJLksuyyhwE8BmAuyfsAbAZwd3m6WBre8s7u6NevX24WbTEdbRMdDSu1tbW5eTQU6Dlx4oSbR1uae0t2Af8o6127uvxj8Q+i4dloKNA7wjt6TKMjuKO+bdq0yc2HDx/u5pUQFr+Z/QZA3m/vzaXtjohUimb4iSRKxS+SKBW/SKJU/CKJUvGLJErFL5KoZLbu3rBhg5tHSzS9cd2hQ4e6baPjnqN5ABFv2Wy0Jfnp06eLuu9o6ap3tHnUNlrqfPjwYTf3RLNNo9t+//333Xzt2rVufv3117t5JeiZXyRRKn6RRKn4RRKl4hdJlIpfJFEqfpFEqfhFEpXMOL83Fg74218DwP79+3OzaPvrCRMmuLk3Fg7Ea+qPHz9eUAbER1VHcxCi+RGXXXZZbhZ9X9E+B9E8AG+vggMHDrhto/X+I0eOdPPRo0e7eS3QM79IolT8IolS8YskSsUvkigVv0iiVPwiiVLxiyQqmXH+adOmufkLL7zg5t6Rzg8++KDb9qGH/AOMo+Oio3kEnmi9/o4dO4q672gewaFDh3KzaI5AsfMfvH39o/X0zc3Nbh6d1XD06FE3rwV65hdJlIpfJFEqfpFEqfhFEqXiF0mUil8kUSp+kUSF4/wkhwN4FsDlANoAzDazJ0k+CuAvAbRmV33YzF4pV0eLtXHjRjf31usD/rr36dOnu23nzZvn5rfddpubR+cCePmwYcPcttGZAlHfo7H2K6+8Mjerq6tz20Zr6qOfmTdWP3XqVLftkiVL3LyxsdHNve+7VnRnks8pAN8ys6Uk6wAsIflalj1uZv9Svu6JSLmExW9mLQBass8PkVwDwH86EZGad1av+UmOAHANgMXZRd8guYLk0yS7/LuY5CySTSSbWltbu7qKiFRBt4uf5AAAzwP4ppkdBPBDAKMATED7Xwbf66qdmc02s0Yza4zORxORyulW8ZPsjfbC/4mZvQAAZrbTzE6bWRuApwBMLF83RaTUwuJn+/atcwCsMbPvd7q8odPVvghgVem7JyLlwmiohuT1AN4CsBLtQ30A8DCAGWj/k98ANAP4WvbmYK7GxkZramoqssuF8bZxBoBt27a5ubf0dcqUKQX1SaTUGhsb0dTU1K0z37vzbv9vAHR1YzU7pi8iMc3wE0mUil8kUSp+kUSp+EUSpeIXSZSKXyRR58zW3adOnXLz6Aju6MhlL4+WnkZ5tD12MVtYR0dsR7cdtY8edy+PfibR49aNOSq5WbSUudjvO3pce/fu7eaVoGd+kUSp+EUSpeIXSZSKXyRRKn6RRKn4RRKl4hdJVLiev6R3RrYC2NTponoAuyvWgbNTq32r1X4B6luhStm3PzGzbu2XV9Hi/8idk01m5m+AXiW12rda7RegvhWqWn3Tn/0iiVLxiySq2sU/u8r376nVvtVqvwD1rVBV6VtVX/OLSPVU+5lfRKpExS+SqKoUP8lpJNeR3EDyoWr0IQ/JZpIrSS4jWZ1DBv7Yl6dJ7iK5qtNll5J8jeTvso/5Z4dXvm+PktyWPXbLSPpnl5evb8NJ/prkGpKrSf5NdnlVHzunX1V53Cr+mp9kLwDrAdwCYCuAdwHMMLP3KtqRHCSbATSaWdUnhJC8EcBhAM+a2fjssn8GsNfMHsv+4xxoZt+ukb49CuBwtY9tz06Tauh8rDyAuwB8FVV87Jx+fQlVeNyq8cw/EcAGM9toZicB/AzAnVXoR80zs4UA9p5x8Z0Ansk+fwbtvzwVl9O3mmBmLWa2NPv8EICOY+Wr+tg5/aqKahT/MABbOn29FVV8ALpgAH5FcgnJWdXuTBeGdByLln0cXOX+nCk8tr2SzjhWvmYeu0KOuy+1ahR/V5uj1dJ446fM7FoAtwP4evbnrXRPt45tr5QujpWvCYUed19q1Sj+rQCGd/r6CgDbq9CPLpnZ9uzjLgC/QO0dPb6z44Tk7OOuKvfnD2rp2PaujpVHDTx2tXTcfTWK/10AV5O8kmQfAF8G8HIV+vERJPtnb8SAZH8At6L2jh5/GcDM7POZAF6qYl8+pFaObc87Vh5Vfuxq7bj7qszwy4YyngDQC8DTZvadineiCyRHov3ZHmjf1vyn1ewbyecA3IT2JZ87ATwC4EUAcwF8DMBmAHebWcXfeMvp2004y2Pby9S3vGPlF6OKj10pj7svSX80vVckTZrhJ5IoFb9IolT8IolS8YskSsUvkigVv0iiVPwiifp/vjpMiBshkooAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = 5\n",
    "plt.imshow(train[:,idx].reshape(28,28), cmap = matplotlib.cm.binary, interpolation = \"nearest\")\n",
    "plt.axis(\"on\")\n",
    "plt.title(get_label_name(labels[:,idx]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 layer with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_layers(number_neurons):\n",
    "    n_dim = 784\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Number of neurons in the layers\n",
    "    n1 = number_neurons # Number of neurons in layer 1\n",
    "    n2 = number_neurons # Number of neurons in layer 2 \n",
    "    n3 = number_neurons\n",
    "    n4 = 10\n",
    "    #n5 = 10 # Neurons for the softmax function\n",
    "\n",
    "    cost_history = np.empty(shape=[0], dtype = float)\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=())\n",
    "\n",
    "    stddev_f = 0.1\n",
    "\n",
    "    tf.set_random_seed(5)\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [n_dim, None])\n",
    "    Y = tf.placeholder(tf.float32, [10, None])\n",
    "    W1 = tf.Variable(tf.random_normal([n1, n_dim], stddev=stddev_f)) \n",
    "    b1 = tf.Variable(tf.constant(0.0, shape = [n1,1]) )\n",
    "    W2 = tf.Variable(tf.random_normal([n2, n1], stddev=stddev_f)) \n",
    "    b2 = tf.Variable(tf.constant(0.0, shape = [n2,1])) \n",
    "    W3 = tf.Variable(tf.random_normal([n3,n2], stddev = stddev_f))\n",
    "    b3 = tf.Variable(tf.constant(0.0, shape = [n3,1]))\n",
    "    W4 = tf.Variable(tf.random_normal([n4,n3], stddev = stddev_f))\n",
    "    b4 = tf.Variable(tf.constant(0.0, shape = [n4,1]))\n",
    "    #W5 = tf.Variable(tf.truncated_normal([n5,n4], stddev = stddev_f))\n",
    "    #b5 = tf.Variable(tf.constant(stddev_f, shape = [n5,1]))\n",
    "\n",
    "    # Let's build our network...\n",
    "    Z1 = tf.nn.relu(tf.matmul(W1, X) + b1) # n1 x n_dim * n_dim x n_obs = n1 x n_obs\n",
    "    Z2 = tf.nn.relu(tf.matmul(W2, Z1) + b2) # n2 x n1 * n1 * n_obs = n2 x n_obs\n",
    "    Z3 = tf.nn.relu(tf.matmul(W3, Z2) + b3)\n",
    "    Z4 = tf.matmul(W4, Z3) + b4\n",
    "    #Z4 = tf.nn.relu(tf.matmul(W4, Z3) + b4)\n",
    "    #Z5 = tf.matmul(W5,Z4) + b5\n",
    "    y_ = tf.nn.softmax(Z4,0) # n2 x n_obs (10 x None)\n",
    "\n",
    "\n",
    "    cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    return optimizer, cost, y_, X, Y, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(number_neurons):\n",
    "    n_dim = 784\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Number of neurons in the layers\n",
    "    n1 = number_neurons# Number of neurons in layer 1\n",
    "    n2 = 10 # Number of neurons in output layer \n",
    "\n",
    "    cost_history = np.empty(shape=[1], dtype = float)\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=())\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [n_dim, None])\n",
    "    Y = tf.placeholder(tf.float32, [10, None])\n",
    "    W1 = tf.Variable(tf.truncated_normal([n1, n_dim], stddev=.1)) \n",
    "    b1 = tf.Variable(tf.constant(0.1, shape = [n1,1]) )\n",
    "    W2 = tf.Variable(tf.truncated_normal([n2, n1], stddev=.1)) \n",
    "    b2 = tf.Variable(tf.constant(0.1, shape = [n2,1])) \n",
    "\n",
    "    # Let's build our network...\n",
    "    Z1 = tf.nn.relu(tf.matmul(W1, X) + b1) # n1 x n_dim * n_dim x n_obs = n1 x n_obs\n",
    "    Z2 = tf.matmul(W2, Z1) + b2 # n2 x n1 * n1 * n_obs = n2 x n_obs\n",
    "    y_ = tf.nn.softmax(Z2,0) # n2 x n_obs (10 x None)\n",
    "\n",
    "    cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    return optimizer, cost, y_, X, Y, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_layers(minibatch_size, training_epochs, features, classes, logging_step = 100, \n",
    "                 learning_r = 0.001, number_neurons = 15, debug = False):\n",
    "    \n",
    "    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    cost_history = []\n",
    "    for epoch in range(training_epochs+1):\n",
    "        for i in range(0, features.shape[1], minibatch_size):\n",
    "            X_train_mini = features[:,i:i + minibatch_size]\n",
    "            y_train_mini = classes[:,i:i + minibatch_size]\n",
    "\n",
    "            #if (i % 5000 == 0):\n",
    "            #    print('i = ',i)\n",
    "            \n",
    "            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})\n",
    "        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})\n",
    "        cost_history = np.append(cost_history, cost_)\n",
    "\n",
    "        if ((epoch % logging_step == 0) & debug):\n",
    "                print(\"Reached epoch\",epoch,\"cost J =\", cost_)\n",
    "                \n",
    "    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "    accuracy_train = accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess)\n",
    "    accuracy_test = accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess)\n",
    "                \n",
    "    return accuracy_train, accuracy_test, sess, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(minibatch_size, training_epochs, features, classes, logging_step = 100, learning_r = 0.001, number_neurons = 15):\n",
    "    \n",
    "    opt, c, y_, X, Y, learning_rate = build_model(number_neurons)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    cost_history = []\n",
    "    for epoch in range(training_epochs+1):\n",
    "        for i in range(0, features.shape[1], minibatch_size):\n",
    "            X_train_mini = features[:,i:i + minibatch_size]\n",
    "            y_train_mini = classes[:,i:i + minibatch_size]\n",
    "\n",
    "            #if (i % 5000 == 0):\n",
    "            #    print('i = ',i)\n",
    "            \n",
    "            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})\n",
    "        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})\n",
    "        cost_history = np.append(cost_history, cost_)\n",
    "\n",
    "        if (epoch % logging_step == 0):\n",
    "                print(\"Reached epoch\",epoch,\"cost J =\", cost_)\n",
    "                \n",
    "    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "    accuracy_train = accuracy.eval({X: train, Y: labels_, learning_rate: learning_r}, session = sess)\n",
    "    accuracy_test = accuracy.eval({X: test, Y: labels_test_, learning_rate: learning_r}, session = sess)\n",
    "                \n",
    "    return accuracy_train, accuracy_test, sess, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached epoch 0 cost J = 0.32642397\n",
      "Reached epoch 10 cost J = 0.30189896\n",
      "Reached epoch 20 cost J = 0.24828354\n",
      "Reached epoch 30 cost J = 0.19529264\n",
      "Reached epoch 40 cost J = 0.16858627\n",
      "Reached epoch 50 cost J = 0.15351932\n",
      "0.6648\n",
      "0.6616\n",
      "Wall time: 1min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "acc_train, acc_test, sess, cost_history = model(minibatch_size = 50, \n",
    "                              training_epochs = 50, \n",
    "                              features = train, \n",
    "                              classes = labels_, \n",
    "                              logging_step = 10,\n",
    "                              learning_r = 0.001,\n",
    "                              number_neurons = 15)\n",
    "\n",
    "print(acc_train)\n",
    "print(acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "Using grid search find the optimal number of neurons in the layer that gives you the best accuracy. With optimal is meant big enough but not too big. After a certain number, increasing the number will not help anymore. Try to finda  good balance between number of neurons and time required for training training.\n",
    "\n",
    "You can use a code similar to the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = [1,5,10,15,25,30, 50, 150]\n",
    "for nn_ in nn:\n",
    "    acc_train, acc_test, sess, cost_history = model(minibatch_size = 50, \n",
    "                              training_epochs = 50, \n",
    "                              features = train, \n",
    "                              classes = labels_, \n",
    "                              logging_step = 50,\n",
    "                              learning_r = 0.001,\n",
    "                              number_neurons = nn_)\n",
    "    print('Number:',nn_,'Acc. Train:', acc_train, 'Acc. Test', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Find the best learning rate and number of neurons with random and logarithmic search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyperparameter tuning for\n",
    "\n",
    "- Learning rate\n",
    "- Number of neurons\n",
    "\n",
    "using random search and for the learning rate logarithimc search. \n",
    "\n",
    "You can use the following code as example.\n",
    "\n",
    "- Try the code with a different number of randomly selected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_ = np.random.randint(low=35, high=60.0, size=(10))\n",
    "\n",
    "r = -np.random.random([10])*4.0\n",
    "\n",
    "learning_ = 10**r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.17655096e-04, 5.86204655e-02, 1.04542655e-02, 2.23468096e-04,\n",
       "       6.66085557e-01, 1.90703727e-01, 9.20111902e-03, 4.61827071e-01,\n",
       "       5.22271634e-02, 7.82464921e-03])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neurons_)):\n",
    "    acc_train, acc_test, sess, cost_history = model_layers(minibatch_size = 50, \n",
    "                              training_epochs = 50, \n",
    "                              features = train, \n",
    "                              classes = labels_, \n",
    "                              logging_step = 50,\n",
    "                              learning_r = learning_[i],\n",
    "                              number_neurons = neurons_[i], debug = False)\n",
    "    print('Number:',neurons_[i],'learning', learning_[i], 'Acc. Train:', acc_train, 'Acc. Test', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Optimise learning rate, number of neurons and mini-batch size (with random search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do hyperparameter tuning for\n",
    "\n",
    "- Learning rate\n",
    "- Number of neurons\n",
    "- mini-batch size\n",
    "\n",
    "using random search and for the learning rate logarithimc search. \n",
    "\n",
    "You can use the following code as example.\n",
    "\n",
    "- Try the code with a different number of randomly selected values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_ = np.random.randint(low=35, high=60.0, size=(5))\n",
    "\n",
    "r = -np.random.random([10])*4.0\n",
    "\n",
    "learning_ = 10**r\n",
    "\n",
    "mb_size_ = np.random.randint(low=20, high=80, size = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66, 40, 70, 71, 51])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb_size_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(neurons_)):\n",
    "    #print('Number:',neurons_[i],'learning', learning_[i])\n",
    "    acc_train, acc_test, sess, cost_history = model_layers(minibatch_size = mb_size_[i], \n",
    "                              training_epochs = 50, \n",
    "                              features = train, \n",
    "                              classes = labels_, \n",
    "                              logging_step = 50,\n",
    "                              learning_r = learning_[i],\n",
    "                              number_neurons = neurons_[i], debug = False)\n",
    "    print('Number:',neurons_[i],'learning', learning_[i], 'mb size',mb_size_[i],\n",
    "          'Acc. Train:', acc_train, 'Acc. Test', acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's test it with Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_layers(number_neurons):\n",
    "    n_dim = 784\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "    \n",
    "    # Number of neurons in the layers\n",
    "    n1 = number_neurons # Number of neurons in layer 1\n",
    "    n2 = number_neurons # Number of neurons in layer 2 \n",
    "    n3 = number_neurons\n",
    "    n4 = 10\n",
    "\n",
    "    cost_history = np.empty(shape=[0], dtype = float)\n",
    "    learning_rate = tf.placeholder(tf.float32, shape=())\n",
    "\n",
    "    stddev_f = 0.1\n",
    "\n",
    "    tf.set_random_seed(5)\n",
    "\n",
    "    X = tf.placeholder(tf.float32, [n_dim, None])\n",
    "    Y = tf.placeholder(tf.float32, [10, None])\n",
    "    W1 = tf.Variable(tf.random_normal([n1, n_dim], stddev=stddev_f)) \n",
    "    b1 = tf.Variable(tf.constant(0.0, shape = [n1,1]) )\n",
    "    W2 = tf.Variable(tf.random_normal([n2, n1], stddev=stddev_f)) \n",
    "    b2 = tf.Variable(tf.constant(0.0, shape = [n2,1])) \n",
    "    W3 = tf.Variable(tf.random_normal([n3,n2], stddev = stddev_f))\n",
    "    b3 = tf.Variable(tf.constant(0.0, shape = [n3,1]))\n",
    "    W4 = tf.Variable(tf.random_normal([n4,n3], stddev = stddev_f))\n",
    "    b4 = tf.Variable(tf.constant(0.0, shape = [n4,1]))\n",
    "    #W5 = tf.Variable(tf.truncated_normal([n5,n4], stddev = stddev_f))\n",
    "    #b5 = tf.Variable(tf.constant(stddev_f, shape = [n5,1]))\n",
    "\n",
    "    # Let's build our network...\n",
    "    Z1 = tf.nn.relu(tf.matmul(W1, X) + b1) # n1 x n_dim * n_dim x n_obs = n1 x n_obs\n",
    "    Z2 = tf.nn.relu(tf.matmul(W2, Z1) + b2) # n2 x n1 * n1 * n_obs = n2 x n_obs\n",
    "    Z3 = tf.nn.relu(tf.matmul(W3, Z2) + b3)\n",
    "    Z4 = tf.matmul(W4, Z3) + b4\n",
    "    #Z4 = tf.nn.relu(tf.matmul(W4, Z3) + b4)\n",
    "    #Z5 = tf.matmul(W5,Z4) + b5\n",
    "    y_ = tf.nn.softmax(Z4,0) # n2 x n_obs (10 x None)\n",
    "\n",
    "\n",
    "    cost = - tf.reduce_mean(Y * tf.log(y_)+(1-Y) * tf.log(1-y_))\n",
    "    optimizer = optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate, \n",
    "                                                   beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8).minimize(cost)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    return optimizer, cost, y_, X, Y, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_layers(minibatch_size, training_epochs, features, classes, logging_step = 100, \n",
    "                 learning_r = 0.001, number_neurons = 15, debug = False):\n",
    "    \n",
    "    opt, c, y_, X, Y, learning_rate = build_model_layers(number_neurons)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    cost_history = []\n",
    "    for epoch in range(training_epochs+1):\n",
    "        for i in range(0, features.shape[1], minibatch_size):\n",
    "            X_train_mini = features[:,i:i + minibatch_size]\n",
    "            y_train_mini = classes[:,i:i + minibatch_size]\n",
    "\n",
    "            #if (i % 5000 == 0):\n",
    "            #    print('i = ',i)\n",
    "            \n",
    "            sess.run(opt, feed_dict = {X: X_train_mini, Y: y_train_mini, learning_rate: learning_r})\n",
    "        cost_ = sess.run(c, feed_dict={ X:features, Y: classes, learning_rate: learning_r})\n",
    "        cost_history = np.append(cost_history, cost_)\n",
    "\n",
    "        if ((epoch % logging_step == 0) & debug):\n",
    "                print(\"Reached epoch\",epoch,\"cost J =\", cost_)\n",
    "                \n",
    "    correct_predictions = tf.equal(tf.argmax(y_,0), tf.argmax(Y,0))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n",
    "    accuracy_train = accuracy.eval({X: train, Y: labels_, learning_rate: 0.001}, session = sess)\n",
    "    accuracy_test = accuracy.eval({X: test, Y: labels_test_, learning_rate: 0.001}, session = sess)\n",
    "                \n",
    "    return accuracy_train, accuracy_test, sess, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_ = np.random.randint(low=35, high=60.0, size=(20))\n",
    "\n",
    "r = -np.random.random([20])*(6-5)-5\n",
    "\n",
    "learning_ = 10**r\n",
    "\n",
    "mb_size_ = np.random.randint(low=20, high=80, size = 20)\n",
    "\n",
    "epochs_ = np.random.randint(low = 40, high = 100, size = (20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 65 Number: 46 learning 3.75743361986e-06 mb size 20 Acc. Train: 0.838683 Acc. Test 0.8407\n",
      "epochs: 54 Number: 37 learning 7.12390609733e-06 mb size 56 Acc. Train: 0.8246 Acc. Test 0.8241\n",
      "epochs: 42 Number: 37 learning 2.14766290607e-06 mb size 45 Acc. Train: 0.712933 Acc. Test 0.7131\n",
      "epochs: 70 Number: 39 learning 5.59340704666e-06 mb size 76 Acc. Train: 0.814483 Acc. Test 0.8107\n",
      "epochs: 83 Number: 50 learning 8.14470677233e-06 mb size 32 Acc. Train: 0.1 Acc. Test 0.1\n",
      "epochs: 60 Number: 44 learning 3.97668096387e-06 mb size 58 Acc. Train: 0.8123 Acc. Test 0.815\n",
      "epochs: 52 Number: 35 learning 2.11130468914e-06 mb size 78 Acc. Train: 0.691283 Acc. Test 0.6906\n",
      "epochs: 70 Number: 43 learning 4.94147290757e-06 mb size 32 Acc. Train: 0.837283 Acc. Test 0.8374\n",
      "epochs: 70 Number: 44 learning 8.09986058619e-06 mb size 59 Acc. Train: 0.847767 Acc. Test 0.8477\n",
      "epochs: 46 Number: 35 learning 3.28760183015e-06 mb size 37 Acc. Train: 0.779567 Acc. Test 0.777\n",
      "epochs: 51 Number: 46 learning 3.11646009783e-06 mb size 41 Acc. Train: 0.805367 Acc. Test 0.8057\n",
      "epochs: 96 Number: 58 learning 2.05750793342e-06 mb size 49 Acc. Train: 0.820183 Acc. Test 0.8206\n",
      "epochs: 61 Number: 54 learning 1.30537893914e-06 mb size 38 Acc. Train: 0.77015 Acc. Test 0.768\n",
      "epochs: 72 Number: 45 learning 2.88601092474e-06 mb size 52 Acc. Train: 0.8057 Acc. Test 0.8053\n",
      "epochs: 94 Number: 52 learning 5.31016599e-06 mb size 37 Acc. Train: 0.1 Acc. Test 0.1\n",
      "epochs: 84 Number: 54 learning 2.97351049253e-06 mb size 43 Acc. Train: 0.832483 Acc. Test 0.831\n",
      "epochs: 42 Number: 36 learning 8.22904299748e-06 mb size 25 Acc. Train: 0.836567 Acc. Test 0.8364\n",
      "epochs: 70 Number: 50 learning 2.24455609222e-06 mb size 34 Acc. Train: 0.81065 Acc. Test 0.8111\n",
      "epochs: 93 Number: 37 learning 2.90045439769e-06 mb size 28 Acc. Train: 0.82645 Acc. Test 0.8278\n",
      "epochs: 82 Number: 36 learning 6.71104798403e-06 mb size 72 Acc. Train: 0.8331 Acc. Test 0.8329\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(neurons_)):\n",
    "    #print('Number:',neurons_[i],'learning', learning_[i])\n",
    "    acc_train, acc_test, sess, cost_history = model_layers(minibatch_size = mb_size_[i], \n",
    "                              training_epochs = epochs_[i], \n",
    "                              features = train, \n",
    "                              classes = labels_, \n",
    "                              logging_step = 10,\n",
    "                              learning_r = learning_[i],\n",
    "                              number_neurons = neurons_[i], debug = False)\n",
    "    print('epochs:', epochs_[i], 'Number:',neurons_[i],'learning', learning_[i], 'mb size',mb_size_[i],\n",
    "          'Acc. Train:', acc_train, 'Acc. Test', acc_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
